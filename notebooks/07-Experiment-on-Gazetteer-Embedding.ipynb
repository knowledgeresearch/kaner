{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Experiment: Gazetteer Embedding\n",
    "This notebook evaluate the test set for the task `Gazetteer Embedding`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.adapter.in_adapter import split_dataset\n",
    "from kaner.adapter.out_adapter import BaseOutAdapter\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.tracker import NERTracker, NERTrackerRow\n",
    "from kaner.common import load_json, load_jsonl, save_json\n",
    "from kaner.common.func import query_time\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TrainerConfig, has_embeddings: bool) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a configuration, train a model on a dataset with gazetteer modification.\n",
    "\n",
    "    Args:\n",
    "        config (TrainerConfig): Trainer Configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_hyperparameters(tokenizer: CharTokenizer, out_adapter: BaseOutAdapter, gazetteer: Gazetteer):\n",
    "        \"\"\"\n",
    "        Update hyper parameters.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (CharTokenizer): Tokenizer.\n",
    "            out_adapter (BaseOutAdapter): Output adapter.\n",
    "            gazetteer (Gazetteer): Gazetteer.\n",
    "        \"\"\"\n",
    "        partial_configs = {\"n_tags\": len(out_adapter)}\n",
    "        partial_configs.update(tokenizer.configs())\n",
    "        partial_configs.update(gazetteer.configs())\n",
    "\n",
    "        return partial_configs\n",
    "\n",
    "    raw_datasets = split_dataset(config.dataset_folder, dataset_pp=config.dataset_pp)\n",
    "    tokenizer = CharTokenizer(config.tokenizer_model_folder)\n",
    "    tokenizer.save(config.output_folder)\n",
    "    gazetteer = Gazetteer(config.gazetteer_model_folder)\n",
    "    gazetteer.save(config.output_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=config.dataset_folder, file_name=\"labels\")\n",
    "    out_adapter.save(config.output_folder, \"labels\")\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in raw_datasets\n",
    "    )\n",
    "    token_embeddings = tokenizer.embeddings()\n",
    "    if has_embeddings:\n",
    "        lexicon_embeddings = gazetteer.embeddings()\n",
    "    else:\n",
    "        lexicon_embeddings = None\n",
    "    config.hyperparameters = update_hyperparameters(tokenizer, out_adapter, gazetteer)\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters, token_embeddings=token_embeddings, lexicon_embeddings=lexicon_embeddings)\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, nn.CrossEntropyLoss(),\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    results = trainer.train()\n",
    "\n",
    "    return results, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainall(labpath: str, cfgdir: str, m: List[str], d: List[str], n: int, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Experiments for all model's training.\n",
    "\n",
    "    Args:\n",
    "        labpath (str): The file path of recording experimental results.\n",
    "        cfgdir (str): Configuration folder.\n",
    "        m (List[str]): All specific models to be trained.\n",
    "        d (List[str]): All specific datasets to be tested.\n",
    "        n (int): The number of training repeating times.\n",
    "        tag (str): Experimental tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_names(names: List[str], all_names: List[str], name_type: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check whether the name that user inputs is correct.\n",
    "\n",
    "        Args:\n",
    "            names (List[str]): The names (dataset, model, gazetteer) that user inputs.\n",
    "            all_names (List[str]): All names (dataset, model, gazetteer) that this libary provides.\n",
    "            name_type (str): The type of the name (Dataset, Model, Gazetteer).\n",
    "        \"\"\"\n",
    "        if len(names) == 0:\n",
    "            names = all_names\n",
    "        else:\n",
    "            for name in names:\n",
    "                if name not in all_names:\n",
    "                    print(\"[{0}] {1} is not in {2}\".format(name_type, name, all_names))\n",
    "                    exit(0)\n",
    "        return names\n",
    "\n",
    "    tracker = NERTracker.load(labpath)\n",
    "    models = update_names(m, gctx.get_model_names(), \"Model\")\n",
    "    datasets = update_names(d, gctx.get_dataset_names(), \"Dataset\")\n",
    "\n",
    "    print(\"--------------------- Laboratory Configuration ---------------------\")\n",
    "    print(\"Models: {0}\".format(models))\n",
    "    print(\"Datasets: {0}\".format(datasets))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            for gazetteer_model in [\"gigaword\", \"sgns\"]:\n",
    "                for _ in range(n):\n",
    "                    for has_embeddings in [True, False]:\n",
    "                        tag = \"has_embeddings:{0}\".format(has_embeddings)\n",
    "                        if len(tracker.query(dataset=dataset, model=model, gazetteer_model=gazetteer_model, tag=tag)) >= n:\n",
    "                            continue\n",
    "                        config = TrainerConfig(os.path.join(cfgdir, model + \".yml\"), dataset=dataset, gazetteer_model=gazetteer_model, **kwargs)\n",
    "                        start = str(datetime.now())\n",
    "                        try:\n",
    "                            results, trainer = train(config, has_embeddings)\n",
    "                        except RuntimeError as error:\n",
    "                            print(error)\n",
    "                            continue\n",
    "                        tracker.insert(\n",
    "                            NERTrackerRow(\n",
    "                                start, model, dataset, config.tokenizer_model, gazetteer_model, config.output_folder, query_time(trainer.train),\n",
    "                                results[\"f1-score\"], results[\"precision-score\"], results[\"recall-score\"], results[\"epoch_count\"], results[\"test-loss\"], tag\n",
    "                            )\n",
    "                        )\n",
    "                        tracker.save(labpath)\n",
    "                        del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labpath = \"../data/logs/experiments_gazetteer_embedding.csv\"\n",
    "cfgdir = \"../configs\"\n",
    "models = [\"ses\", \"cgn\", \"mdgg\"]\n",
    "datasets = [\"weiboner\"]\n",
    "n = 5\n",
    "kwargs = {\"data_folder\": \"../data\", \"gpu\": [1]}\n",
    "\n",
    "trainall(labpath, cfgdir, models, datasets, n, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
