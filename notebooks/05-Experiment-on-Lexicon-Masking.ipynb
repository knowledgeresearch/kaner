{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Experiment: Lexicon Masking\n",
    "This notebook evaluate the test set for the task `Lexicon Masking`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.adapter.in_adapter import split_dataset\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.common import load_json, load_jsonl, save_json\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matlexs(datasets: List[dict], gazetteer: Gazetteer, mode: str) -> List[set]:\n",
    "    \"\"\"\n",
    "    Given a matching mode, return all matched lexicons.\n",
    "    \"\"\"\n",
    "    max_seq_len = 512\n",
    "    assert mode in [\"all\", \"entity\", \"non-entity\"]\n",
    "    # get all spans\n",
    "    all_spans = set()\n",
    "    for dataset in datasets:\n",
    "        for datapoint in dataset:\n",
    "            for span in datapoint[\"spans\"]:\n",
    "                all_spans.add(span[\"text\"])\n",
    "    # get all matched lexicons\n",
    "    matched_lexicons = []\n",
    "    for dataset in datasets:\n",
    "        lexicons = set()\n",
    "        for datapoint in dataset:\n",
    "            tokens = list(datapoint[\"text\"])[:max_seq_len]\n",
    "            for i, _ in enumerate(tokens):\n",
    "                items = gazetteer.search(tokens[i:])\n",
    "                if mode == \"all\":\n",
    "                    lexicons.update(items)\n",
    "                else:\n",
    "                    for item in items:\n",
    "                        if mode == \"entity\":\n",
    "                            if item in all_spans:\n",
    "                                lexicons.add(item)\n",
    "                        elif mode == \"non-entity\":\n",
    "                            if item not in all_spans:\n",
    "                                lexicons.add(item)\n",
    "        matched_lexicons.append(lexicons)\n",
    "    \n",
    "    return matched_lexicons\n",
    "\n",
    "\n",
    "def evaluate(model_folder: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate all settings.\n",
    "    \"\"\"\n",
    "    options = load_json(\"utf-8\", model_folder, \"config.json\")\n",
    "    options[\"output_folder\"] = model_folder\n",
    "    options[\"identity\"] = os.path.basename(os.path.normpath(model_folder))\n",
    "    config = TrainerConfig(options, data_folder=\"../data\")\n",
    "    tokenizer = CharTokenizer(model_folder)\n",
    "    gazetteer = Gazetteer(model_folder)\n",
    "    datasets = split_dataset(config.dataset_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=model_folder, file_name=\"labels\")\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters)\n",
    "\n",
    "    result = {}\n",
    "    # IS Intervention\n",
    "    A, _, B = get_matlexs(datasets, gazetteer, \"all\")\n",
    "    I = A.intersection(B)\n",
    "    S = A.union(B) - A\n",
    "    gazetteer.mask(list(I), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"I\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(I), False)\n",
    "\n",
    "    gazetteer.mask(list(S), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"S\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(S), False)\n",
    "\n",
    "    # Entity vs. Non-Entity\n",
    "    _, _, E = get_matlexs(datasets, gazetteer, \"entity\")\n",
    "    _, _, N = get_matlexs(datasets, gazetteer, \"non-entity\")\n",
    "    gazetteer.mask(list(E), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"E\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(E), False)\n",
    "\n",
    "    gazetteer.mask(list(N), True)\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in [[], [], datasets[2]]\n",
    "    )\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, None,\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    trainer.startp()\n",
    "    result[\"N\"] = trainer._test(trainer._test_loader)\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gazetteer.mask(list(N), False)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute `do` Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(folder: str = \"../data/logs\") -> List[dict]:\n",
    "    file_path = os.path.join(folder, \"experiments.csv\")\n",
    "    logs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        line = fin.readline()\n",
    "        columns = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            log = {k: v for k, v in zip(columns, line.replace(\"\\n\", \"\").split(\",\"))}\n",
    "            if log[\"model\"] not in [\"blcrf\", \"plmtg\"]:\n",
    "                logs.append(log)\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "dolog_path = os.path.join(\"../data\", \"do_full_logs.json\")\n",
    "if os.path.isfile(dolog_path):\n",
    "    logs = load_json(\"utf-8\", dolog_path)\n",
    "else:\n",
    "    logs = load_experiments()\n",
    "for i, _ in enumerate(logs):\n",
    "    print(\"## Log {0}...........................\".format(i))\n",
    "    if \"do\" in logs[i].keys():\n",
    "        continue\n",
    "    if not logs[i][\"log_dir\"].startswith(\"../\"):\n",
    "        folder = os.path.join(\"../\", logs[i][\"log_dir\"])\n",
    "    else:\n",
    "        folder = logs[i][\"log_dir\"]\n",
    "    folder = folder.replace(\"tmp/\", \"\")\n",
    "    logs[i][\"do\"] = evaluate(folder)\n",
    "    save_json(logs, dolog_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
