{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Experiment: Gazetteer Length Group\n",
    "This notebook evaluate the test set for the task `Gazetteer Length Group`.\n",
    "\n",
    "**Note**: Before conducting experiments, you need to install `kaner` package first. Otherwise, this notebook will raise an *import error*.\n",
    "\n",
    "```bash\n",
    "cd ../\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from kaner.context import GlobalContext as gctx\n",
    "from kaner.adapter.tokenizer import CharTokenizer\n",
    "from kaner.adapter.knowledge import Gazetteer\n",
    "from kaner.adapter.in_adapter import split_dataset\n",
    "from kaner.adapter.out_adapter import BaseOutAdapter\n",
    "from kaner.trainer import NERTrainer, TrainerConfig\n",
    "from kaner.tracker import NERTracker, NERTrackerRow\n",
    "from kaner.common import load_json, load_jsonl, save_json\n",
    "from kaner.common.func import query_time\n",
    "\n",
    "\n",
    "gctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_gazetteer_by_length(len_start: int, len_end: int, gazetteer_folder: str) -> Gazetteer:\n",
    "    gazetteer = Gazetteer(gazetteer_folder)\n",
    "    lexicons = []\n",
    "    lexicon_sets = defaultdict(list)\n",
    "    with open(os.path.join(gazetteer_folder, \"lexicons.txt\"), \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            lexicon = line.replace(\"\\n\", \"\").split(\"\\t\")[0]\n",
    "            if lexicon == \"[PAD]\":\n",
    "                continue\n",
    "            lexicons.append(lexicon)\n",
    "            lexicon_sets[len(lexicon)].append(lexicon)\n",
    "    remaining_lexicons = set()\n",
    "    for i in range(len_start, len_end + 1):\n",
    "        remaining_lexicons.update(lexicon_sets[i])\n",
    "    # keep the same number of lexicons\n",
    "    remaining_lexicons = set(list(remaining_lexicons)[:200])\n",
    "    masked_lexicons = list(set(lexicons) - remaining_lexicons)\n",
    "    print(\"Total: {0}, Remaining: {1}, Masked: {2}\".format(len(lexicons), len(remaining_lexicons), len(masked_lexicons)))\n",
    "    gazetteer.mask(masked_lexicons, True)\n",
    "\n",
    "    return gazetteer\n",
    "\n",
    "\n",
    "def train(config: TrainerConfig, len_start: int, len_end: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a configuration, train a model on a dataset with gazetteer modification.\n",
    "\n",
    "    Args:\n",
    "        config (TrainerConfig): Trainer Configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_hyperparameters(tokenizer: CharTokenizer, out_adapter: BaseOutAdapter, gazetteer: Gazetteer):\n",
    "        \"\"\"\n",
    "        Update hyper parameters.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (CharTokenizer): Tokenizer.\n",
    "            out_adapter (BaseOutAdapter): Output adapter.\n",
    "            gazetteer (Gazetteer): Gazetteer.\n",
    "        \"\"\"\n",
    "        partial_configs = {\"n_tags\": len(out_adapter)}\n",
    "        partial_configs.update(tokenizer.configs())\n",
    "        partial_configs.update(gazetteer.configs())\n",
    "\n",
    "        return partial_configs\n",
    "\n",
    "    raw_datasets = split_dataset(config.dataset_folder, dataset_pp=config.dataset_pp)\n",
    "    tokenizer = CharTokenizer(config.tokenizer_model_folder)\n",
    "    tokenizer.save(config.output_folder)\n",
    "    gazetteer = get_masked_gazetteer_by_length(len_start, len_end, config.gazetteer_model_folder)\n",
    "    gazetteer.save(config.output_folder)\n",
    "    out_adapter = gctx.create_outadapter(config.out_adapter, dataset_folder=config.dataset_folder, file_name=\"labels\")\n",
    "    out_adapter.save(config.output_folder, \"labels\")\n",
    "    in_adapters = (\n",
    "        gctx.create_inadapter(\n",
    "            config.in_adapter, dataset=dataset, tokenizer=tokenizer, out_adapter=out_adapter, gazetteer=gazetteer,\n",
    "            **config.hyperparameters\n",
    "        )\n",
    "        for dataset in raw_datasets\n",
    "    )\n",
    "    token_embeddings = tokenizer.embeddings()\n",
    "    lexicon_embeddings = gazetteer.embeddings()\n",
    "    config.hyperparameters = update_hyperparameters(tokenizer, out_adapter, gazetteer)\n",
    "    collate_fn = gctx.create_batcher(\n",
    "        config.model, input_pad=tokenizer.pad_id, output_pad=out_adapter.unk_id, lexicon_pad=gazetteer.pad_id, device=config.device\n",
    "    )\n",
    "    model = gctx.create_model(config.model, **config.hyperparameters, token_embeddings=token_embeddings, lexicon_embeddings=lexicon_embeddings)\n",
    "    trainer = NERTrainer(\n",
    "        config, tokenizer, in_adapters, out_adapter, collate_fn, model, nn.CrossEntropyLoss(),\n",
    "        gctx.create_traincallback(config.model), gctx.create_testcallback(config.model)\n",
    "    )\n",
    "    results = trainer.train()\n",
    "\n",
    "    return results, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainall(labpath: str, cfgdir: str, m: List[str], d: List[str], n: int, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Experiments for all model's training.\n",
    "\n",
    "    Args:\n",
    "        labpath (str): The file path of recording experimental results.\n",
    "        cfgdir (str): Configuration folder.\n",
    "        m (List[str]): All specific models to be trained.\n",
    "        d (List[str]): All specific datasets to be tested.\n",
    "        n (int): The number of training repeating times.\n",
    "        tag (str): Experimental tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_names(names: List[str], all_names: List[str], name_type: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check whether the name that user inputs is correct.\n",
    "\n",
    "        Args:\n",
    "            names (List[str]): The names (dataset, model, gazetteer) that user inputs.\n",
    "            all_names (List[str]): All names (dataset, model, gazetteer) that this libary provides.\n",
    "            name_type (str): The type of the name (Dataset, Model, Gazetteer).\n",
    "        \"\"\"\n",
    "        if len(names) == 0:\n",
    "            names = all_names\n",
    "        else:\n",
    "            for name in names:\n",
    "                if name not in all_names:\n",
    "                    print(\"[{0}] {1} is not in {2}\".format(name_type, name, all_names))\n",
    "                    exit(0)\n",
    "        return names\n",
    "\n",
    "    tracker = NERTracker.load(labpath)\n",
    "    models = update_names(m, gctx.get_model_names(), \"Model\")\n",
    "    datasets = update_names(d, gctx.get_dataset_names(), \"Dataset\")\n",
    "\n",
    "    print(\"--------------------- Laboratory Configuration ---------------------\")\n",
    "    print(\"Models: {0}\".format(models))\n",
    "    print(\"Datasets: {0}\".format(datasets))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            for gazetteer_model in [\"gigaword\", \"sgns\", \"tec\"]:\n",
    "                for _ in range(n):\n",
    "                    for len_start, len_end in [(2, 3), (4, 5), (6,7)]:\n",
    "                        tag = \"{0}-{1}\".format(len_start, len_end)\n",
    "                        if len(tracker.query(dataset=dataset, model=model, gazetteer_model=gazetteer_model, tag=tag)) >= n:\n",
    "                            continue\n",
    "                        config = TrainerConfig(os.path.join(cfgdir, model + \".yml\"), dataset=dataset, gazetteer_model=gazetteer_model, **kwargs)\n",
    "                        start = str(datetime.now())\n",
    "                        try:\n",
    "                            results, trainer = train(config, len_start, len_end)\n",
    "                        except RuntimeError as error:\n",
    "                            print(error)\n",
    "                            continue\n",
    "                        tracker.insert(\n",
    "                            NERTrackerRow(\n",
    "                                start, model, dataset, config.tokenizer_model, gazetteer_model, config.output_folder, query_time(trainer.train),\n",
    "                                results[\"f1-score\"], results[\"precision-score\"], results[\"recall-score\"], results[\"epoch_count\"], results[\"test-loss\"], tag\n",
    "                            )\n",
    "                        )\n",
    "                        tracker.save(labpath)\n",
    "                        del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labpath = \"../data/logs/experiments_gazetteer_length_group.csv\"\n",
    "cfgdir = \"../configs\"\n",
    "models = [\"ses\", \"cgn\", \"mdgg\"]\n",
    "datasets = [\"weiboner\"]\n",
    "n = 4\n",
    "kwargs = {\"data_folder\": \"../data\", \"gpu\": [0]}\n",
    "\n",
    "trainall(labpath, cfgdir, models, datasets, n, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
